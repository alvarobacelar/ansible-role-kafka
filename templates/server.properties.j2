# see kafka.server.KafkaConfig for additional details and defaults

# the id of the broker
brokerid=0

# hostname of broker. If not set, will pick up from the value returned
# from getLocalHost.  If there are multiple interfaces getLocalHost
# may not be what you want.
# hostname=

# number of logical partitions on this broker
num.partitions={{ num_partition }}

# the port the socket server runs on
port={{ port_kafka }}

# the number of processor threads the socket server uses. Defaults to the number of cores on the machine
num.threads={{ num_threads }}

# the directory in which to store log files
log.dir={{ log_dir_kafka }}

# the send buffer used by the socket server 
socket.send.buffer=1048576

# the receive buffer used by the socket server
socket.receive.buffer=1048576

# the maximum size of a log segment
log.file.size=536870912

# the interval between running cleanup on the logs
log.cleanup.interval.mins=1

# the minimum age of a log file to eligible for deletion
log.retention.hours=168

#the number of messages to accept without flushing the log to disk
log.flush.interval=1

#set the following properties to use zookeeper

# enable connecting to zookeeper
enable.zookeeper={{ enable_zoo_connect }}

# zk connection string
# comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"
zk.connect={{ zoo_host_server }}:{{ zoo_port_server }}

# timeout in ms for connecting to zookeeper
zk.connectiontimeout.ms=1000000

# time based topic flush intervals in ms
#topic.flush.intervals.ms=topic:1000

# default time based flush interval in ms
log.default.flush.interval.ms=2000

# the interval (in ms) at which logs are checked to see if they need to be flushed to disk.
log.default.flush.scheduler.interval.ms=1000

# topic partition count map
# topic.partition.count.map=topic1:3, topic2:4
